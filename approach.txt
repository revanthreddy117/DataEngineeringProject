Data Engineer:

Description:
This project involves building an application to scrape, store, and visualize data from the 
[NYC TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) Trip Record Data 

Approach Steps:
1.	Web Scraping to download data
2.	Ingesting data to MongoDB
3.	Backend Development for Rest API
4.	Frontend

Tools Required:

Python:  Web scraping and data processing.
MongoDB:  Database storage.
Node.js and Express.js, Mongoose:  Building the backend server and APIs.
HTML, CSS, and JavaScript: Building the frontend interface.

Step 1

1.	Libraries or modules required in this step are (‘requests, bs4, BeautifulSoup, pandas, os, re, and pyarrow’)
2.	Web scraping refers to the automated process of extracting data from websites, typically using scripts or 
        software tools to parse and retrieve information from web pages. It's commonly used for gathering large amounts of data 
        for various purposes such as research, market analysis, or content aggregation.
3.	Website= "https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
4.	From the above mentioned website we have to download 2019, February and march data.
5.	With response of get request if it successful, parsing the content to find parquet files and finding all links 
        to February and March 2019 and downloading them, storing into our directory.
6.	Process the DataFrames to handle missing values, data types, and any necessary transformations.

Step 2

1.	Libraries or modules required in this step `pymongo` for Python-MongoDB interaction
2.	Ingestion of data into MongoDB database cos MongoDB is document-oriented NoSQL database 
        designed to store and manage unstructured data. It doesn’t require predefined schema.
        For SQL databases we have to define schema for every file, so MongoDB removes that need to define schema
3.	Using pymongo library connecting to local MongoDB connection. Ingesting all data of downloaded files into one collection only.

Data Understanding:
By looking at documents in MongoDB we can see what data is present all this parquet files.
'VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',
'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',
'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra',
'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',
'total_amount', 'congestion_surcharge', 'airport_fee'

These are the columns present in yellox_taxi and green_taxi records.

'hvfhs_license_num', 'dispatching_base_num', 'originating_base_num', 'request_datetime'
'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime', 'PULocationID', 'DOLocationID',
'trip_miles', 'trip_time', 'base_passenger_fare', 'tolls', 'bcf', 'sales_tax', 'congestion_surcharge',
'airport_fee', 'tips', 'driver_pay', 'shared_request_flag', 'shared_match_flag', 'access_a_ride_flag',
'wav_request_flag', 'wav_match_flag'

These are the columns present in High Volume For-Hire Vehicle Trip Records, For-Hire Vehicle Trip Records.

So we can understand that data related trip of persons on taxi or ride sharing.

Step 3

1.	Creating REST API, we have used express and NodeJs for Back-End.
2.	We Installed Express.js, Mongoose, and Body-Parser.
3.	We have created 10 APIs.
4.	Some of APIs are Get all data, Get data by date range, Get data by trip distance

Step 4

1.	Front-End, we have used  Html, Css, JavaScript.
2.	Created an HTML file with a table to display the trip data.
3.	Included fields for filtering data.

System Design:

Scraping Component: Python script to scrape, clean, and upload data to MongoDB.
Backend Component: Express.js server with RESTful APIs to interact with the MongoDB database.
                   APIs for fetching data with various filters (date range, passenger count, payment type, etc.).
Frontend Component: HTML page with a table to display the data. CSS for styling the page. 
                    JavaScript for fetching data from the backend APIs and updating the table based on user filters.

